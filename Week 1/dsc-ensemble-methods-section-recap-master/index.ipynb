{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Ensembles - Recap"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Key Takeaways\n", "\n", "The key takeaways from this section include:\n", "\n", "* Multiple independent estimates are consistently more accurate than any single estimate, so ensemble techniques are a powerful way for improving the quality of your models\n", "* Sometimes you'll use model stacking or meta-ensembles where you use a combination of different types of models for your ensemble\n", "* It's also common to have multiple similar models in an ensemble - e.g. a bunch of decision trees\n", "* Bagging (Bootstrap AGGregation) is a technique that leverages Bootstrap Resampling and Aggregation\n", "* Bootstrap resampling uses multiple smaller samples from the test dataset to create independent estimates, and aggregate these estimates to make predictions\n", "* A random forest is an ensemble method for decision trees using Bagging and the Subspace Sampling method to create variance among the trees\n", "* With a random forest, for each tree, we sample two-thirds of the training data and the remaining third is used to calculate the out-of-bag error\n", "* In addition, the Subspace Sampling method is used to further increase variability by randomly selecting the subset of features to use as predictors for training any given tree\n", "* `GridsearchCV` is an exhaustive search technique for finding optimal combinations of hyperparameters \n", "* Boosting leverages an ensemble of weak learners (weak models) to create a strong combined model\n", "* Boosting (when compared to random forests) is an iterative rather than independent process, using each iteration to strengthen the weaknesses of the previous iterations\n", "* Two of the most common algorithms for Boosting are Adaboost (Adaptive Boosting) and Gradient Boosted Trees\n", "* Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive tree\n", "* Gradient Boosting is a more advanced boosting algorithm that makes use of Gradient Descent\n", "* XGBoost (eXtreme Gradient Boosting) is one of the top gradient boosting algorithms currently in use\n", "* `XGBoost` is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.9"}}, "nbformat": 4, "nbformat_minor": 2}